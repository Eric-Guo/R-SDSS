# 第六周作业

## 6.3 线性回归模型

#### 输入数据：
```{r}
df63=data.frame(
X=c(1,1,1,1,2,2,2,3,3,3,
    4,4,4,5,6,6,6,7,7,7,
    8,8,8,9,11,12,12,12),
Y=c(0.6,1.6,0.5,1.2,2.0,1.3,2.5,2.2,2.4,1.2,
    3.5,4.1,5.1,5.7,3.4,9.7,8.6,4.0,5.5,10.5,
    17.5,13.4,4.5,30.4,12.4,13.4,26.2,7.4)
); df63
```

#### 画出散点图及回归直线y=Beta0+Beta1*x，并将回归直线也画在散点图上
```{r}
attach(df63)
plot(Y~X)
lm63.sol <- lm(Y ~ 1 + X, data = df63)
print(lm63.sol)
lines(df63$X, fitted(lm63.sol))
```

#### T检验和F检验

```{r}
summary(lm63.sol)
```

回归常数的p值为0.44且不显著，因此认为不通过T检验

F统计量为30.8，p值为7.93e-6，因此认为通过F检验

#### 画出残差（普通残差和标准化残差）与预测值的残差图

```{r}
yn.res=resid(lm63.sol); 
yn.rst=rstandard(lm63.sol); 
yn.fit=predict(lm63.sol);
plot(yn.res~yn.fit);
plot(yn.rst~yn.fit);
```

#### 分析误差是否是等方差

```{r}
shapiro.test(yn.res);
```

正态检验结果p值0.00131，因此残差不满足正态性假设

#### 修正模型，对响应的变量Y作开方，再做前面三步分析

```{r}
lm63.new=update(lm63.sol, sqrt(.)~.);
plot(sqrt(Y)~X);
print(lm63.new)
lines(df63$X, fitted(lm63.new), col="red")
summary(lm63.new);
```

回归常数和X系数的p值均显著，通过T检验。

F统计量为55.4，p值为6.64e-8，通过F检验。

```{r}
yn.res=resid(lm63.new); 
yn.rst=rstandard(lm63.new); 
yn.fit=predict(lm63.new);
plot(yn.res~yn.fit);
plot(yn.rst~yn.fit);
shapiro.test(yn.res);
```

正态检验结果p值0.4012，能通过正态性检验，因此模型修正是合理的。

## 6.4 牙膏销售数据回归诊断

#### 输入数据

```{r}
toothpaste<-data.frame(
   X1=c(-0.05, 0.25,0.60,0,   0.25,0.20, 0.15,0.05,-0.15, 0.15,
         0.20, 0.10,0.40,0.45,0.35,0.30, 0.50,0.50, 0.40,-0.05,
        -0.05,-0.10,0.20,0.10,0.50,0.60,-0.05,0,    0.05, 0.55),
   X2=c( 5.50,6.75,7.25,5.50,7.00,6.50,6.75,5.25,5.25,6.00,
         6.50,6.25,7.00,6.90,6.80,6.80,7.10,7.00,6.80,6.50,
         6.25,6.00,6.50,7.00,6.80,6.80,6.50,5.75,5.80,6.80),
   Y =c( 7.38,8.51,9.52,7.50,9.33,8.28,8.75,7.87,7.10,8.00,
         7.89,8.15,9.10,8.86,8.90,8.87,9.26,9.00,8.75,7.95,
         7.65,7.27,8.00,8.50,8.75,9.21,8.27,7.67,7.93,9.26)
)
```


#### 引入Reg_Diag函数

```{r}
source("../R-Book-Demo/ch6/Reg_Diag.R", echo=TRUE, max.deparse.length=3e3)
```


#### 进行回归诊断

```{r}
toothpaste.lm1.sol=lm(Y~X1+X2, data=toothpaste)
toothpaste.diag=Reg_Diag(toothpaste.lm1.sol)
print(toothpaste.diag)
```

第5、8号样本的标准化残差、外学生化残差最大，对应变量Y的影响最大。而且DFFITS, Cook, COVRATIO指标超标。

#### 删除异常样本后重新计算

```{r}
toothpaste.lm2.sol=lm(Y~X1+X2, data=toothpaste, subset=c(-5,-8))
summary(toothpaste.lm1.sol)
summary(toothpaste.lm2.sol)
```

5、8号样本删除后对新数据集进行回归后得到的相关系数的R平方已经从0.886上升到0.921，可以认为新模型好于PDF版326页的线性回归模型。

## 6.5 水泥数据的多重共线性诊断

#### 输入数据

```{r}
cement<-data.frame(
   X1=c( 7,  1, 11, 11,  7, 11,  3,  1,  2, 21,  1, 11, 10),
   X2=c(26, 29, 56, 31, 52, 55, 71, 31, 54, 47, 40, 66, 68),
   X3=c( 6, 15,  8,  8,  6,  9, 17, 22, 18,  4, 23,  9,  8),
   X4=c(60, 52, 20, 47, 33, 22,  6, 44, 22, 26, 34, 12, 12),
   Y =c(78.5, 74.3, 104.3,  87.6,  95.9, 109.2, 102.7, 72.5, 
        93.1,115.9,  83.8, 113.3, 109.4)
); cement
```

#### 计算方矩的条件数

```{r}
cement.cor=cor(cement[1:4])
kappa(cement.cor, exact=TRUE)
```

因为kappa条件数为1377>1000，所以可以认为该数据集有严重的多重共线性

#### 计算特征值和特征向量

```{r}
eigen(cement.cor)
```

由于X3，X4前的系数都比较小，可作为去掉变量的候选

#### 尝试去掉X4

```{r}
cement.corX4=cor(cement[,c('X1','X2','X3')])
kappa(cement.corX4, exact=TRUE)
```

得到的kappa条件数是k=11.12 << 1000，多重共线性较弱

#### 尝试去掉X3

```{r}
cement.corX3=cor(cement[,c('X1','X2','X4')])
kappa(cement.corX3, exact=TRUE)
```

得到的kappa条件数是k=77.25 < 1000，多重共线性中等

#### 尝试去掉X3和X4

```{r}
cement.cor2=cor(cement[,c('X1','X2')])
kappa(cement.cor2, exact=TRUE)
```

得到的kappa条件数是k=1.593 << 100，已不具有多重共线性

对比书中例子6.10，step函数自动去掉X3，从多重共线性角度不一定是最优的，但是同时去掉X3，X4从多重共线性分析是很合理的。